{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OeZWL8JyEtED",
    "outputId": "6a8bcd2e-57d9-42c2-ec06-2ffcd298f1ac"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes==0.39.0\n",
      "  Using cached bitsandbytes-0.39.0-py3-none-any.whl.metadata (9.8 kB)\n",
      "Using cached bitsandbytes-0.39.0-py3-none-any.whl (92.2 MB)\n",
      "Installing collected packages: bitsandbytes\n",
      "  Attempting uninstall: bitsandbytes\n",
      "    Found existing installation: bitsandbytes 0.40.2\n",
      "    Uninstalling bitsandbytes-0.40.2:\n",
      "      Successfully uninstalled bitsandbytes-0.40.2\n",
      "Successfully installed bitsandbytes-0.39.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\binh\\miniconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from accelerate) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from accelerate) (23.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\binh\\miniconda3\\lib\\site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\binh\\miniconda3\\lib\\site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\binh\\miniconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\binh\\miniconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\binh\\miniconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\binh\\miniconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\binh\\miniconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://test.pypi.org/simple/\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\binh\\miniconda3\\lib\\site-packages (0.39.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\binh\\miniconda3\\lib\\site-packages (4.31.0)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.52.3-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: accelerate in c:\\users\\binh\\miniconda3\\lib\\site-packages (0.21.0)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\binh\\miniconda3\\lib\\site-packages (0.39.0)\n",
      "Collecting bitsandbytes\n",
      "  Using cached bitsandbytes-0.45.5-py3-none-win_amd64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers) (0.31.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\binh\\miniconda3\\lib\\site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\binh\\miniconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\binh\\miniconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\binh\\miniconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Using cached transformers-4.52.3-py3-none-any.whl (10.5 MB)\n",
      "Using cached accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "Using cached bitsandbytes-0.45.5-py3-none-win_amd64.whl (75.4 MB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Installing collected packages: tokenizers, bitsandbytes, accelerate, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "  Attempting uninstall: bitsandbytes\n",
      "    Found existing installation: bitsandbytes 0.39.0\n",
      "    Uninstalling bitsandbytes-0.39.0:\n",
      "      Successfully uninstalled bitsandbytes-0.39.0\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.21.0\n",
      "    Uninstalling accelerate-0.21.0:\n",
      "      Successfully uninstalled accelerate-0.21.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.31.0\n",
      "    Uninstalling transformers-4.31.0:\n",
      "      Successfully uninstalled transformers-4.31.0\n",
      "Successfully installed accelerate-1.7.0 bitsandbytes-0.45.5 tokenizers-0.21.1 transformers-4.52.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\binh\\miniconda3\\lib\\site-packages (4.52.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers) (0.31.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\binh\\miniconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from requests->transformers) (2022.12.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: trl in c:\\users\\binh\\miniconda3\\lib\\site-packages (0.4.7)\n",
      "Requirement already satisfied: torch>=1.4.0 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from trl) (2.1.2)\n",
      "Requirement already satisfied: transformers>=4.18.0 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from trl) (4.52.3)\n",
      "Requirement already satisfied: numpy>=1.18.2 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from trl) (1.23.5)\n",
      "Requirement already satisfied: accelerate in c:\\users\\binh\\miniconda3\\lib\\site-packages (from trl) (1.7.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\binh\\miniconda3\\lib\\site-packages (from trl) (2.14.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\binh\\miniconda3\\lib\\site-packages (from torch>=1.4.0->trl) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\binh\\miniconda3\\lib\\site-packages (from torch>=1.4.0->trl) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\binh\\miniconda3\\lib\\site-packages (from torch>=1.4.0->trl) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\binh\\miniconda3\\lib\\site-packages (from torch>=1.4.0->trl) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from torch>=1.4.0->trl) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\binh\\miniconda3\\lib\\site-packages (from torch>=1.4.0->trl) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers>=4.18.0->trl) (0.31.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers>=4.18.0->trl) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers>=4.18.0->trl) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers>=4.18.0->trl) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers>=4.18.0->trl) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers>=4.18.0->trl) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers>=4.18.0->trl) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers>=4.18.0->trl) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\binh\\miniconda3\\lib\\site-packages (from accelerate->trl) (5.9.5)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from datasets->trl) (15.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from datasets->trl) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\binh\\miniconda3\\lib\\site-packages (from datasets->trl) (1.5.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\binh\\miniconda3\\lib\\site-packages (from datasets->trl) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\binh\\miniconda3\\lib\\site-packages (from datasets->trl) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\binh\\miniconda3\\lib\\site-packages (from datasets->trl) (3.11.18)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from aiohttp->datasets->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from aiohttp->datasets->trl) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from aiohttp->datasets->trl) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from aiohttp->datasets->trl) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from aiohttp->datasets->trl) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from aiohttp->datasets->trl) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from aiohttp->datasets->trl) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from aiohttp->datasets->trl) (1.20.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from requests->transformers>=4.18.0->trl) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from requests->transformers>=4.18.0->trl) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from requests->transformers>=4.18.0->trl) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from requests->transformers>=4.18.0->trl) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\binh\\miniconda3\\lib\\site-packages (from tqdm>=4.27->transformers>=4.18.0->trl) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from jinja2->torch>=1.4.0->trl) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from pandas->datasets->trl) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from pandas->datasets->trl) (2022.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets->trl) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.28.0\n",
      "  Using cached transformers-4.28.0-py3-none-any.whl.metadata (109 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers==4.28.0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers==4.28.0) (0.31.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers==4.28.0) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers==4.28.0) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers==4.28.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers==4.28.0) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers==4.28.0) (2.32.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.0)\n",
      "  Using cached tokenizers-0.13.3-cp310-cp310-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from transformers==4.28.0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\binh\\miniconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.28.0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from requests->transformers==4.28.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from requests->transformers==4.28.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from requests->transformers==4.28.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\binh\\miniconda3\\lib\\site-packages (from requests->transformers==4.28.0) (2022.12.7)\n",
      "Using cached transformers-4.28.0-py3-none-any.whl (7.0 MB)\n",
      "Using cached tokenizers-0.13.3-cp310-cp310-win_amd64.whl (3.5 MB)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.1\n",
      "    Uninstalling tokenizers-0.21.1:\n",
      "      Successfully uninstalled tokenizers-0.21.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.52.3\n",
      "    Uninstalling transformers-4.52.3:\n",
      "      Successfully uninstalled transformers-4.52.3\n",
      "Successfully installed tokenizers-0.13.3 transformers-4.28.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install needed lib\n",
    "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n",
    "!pip install bitsandbytes==0.39.0\n",
    "!pip install accelerate\n",
    "!pip install -i https://test.pypi.org/simple/ bitsandbytes\n",
    "!pip install --upgrade transformers accelerate bitsandbytes\n",
    "!pip install --upgrade transformers\n",
    "!pip install trl\n",
    "!pip install transformers==4.28.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4rVYQKD_lRgz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\BINH\\miniconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,  # Cẩn thận: dòng này sẽ lỗi nếu bitsandbytes hoặc CUDA lỗi\n",
    "    pipeline,\n",
    "    logging\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vpNAAQt4m8HP",
    "outputId": "fbc103c6-ae5e-4381-9e6f-eb024e67c06e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Gemini-compatible JSONL saved to: medquad_gemini_train.jsonl\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "# Load dataset\n",
    "oa_dataset = load_dataset('keivalya/MedQuad-MedicalQnADataset', split='train')\n",
    "\n",
    "# Shuffle and select subset (for pilot training)\n",
    "oa_dataset = oa_dataset.shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# Convert to Gemini JSONL format\n",
    "def to_gemini_chat_format(example):\n",
    "    question = (example.get(\"Question\") or \"\").strip()\n",
    "    answer = (example.get(\"Answer\") or \"\").strip()\n",
    "\n",
    "    if not question or not answer:\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": answer}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Apply transformation\n",
    "formatted_data = [to_gemini_chat_format(ex) for ex in oa_dataset if to_gemini_chat_format(ex) is not None]\n",
    "\n",
    "# Save as JSONL\n",
    "output_path = \"medquad_gemini_train.jsonl\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in formatted_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"✅ Gemini-compatible JSONL saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90% train / 10% validation split\n",
    "split_idx = int(len(formatted_data) * 0.9)\n",
    "train_data = formatted_data[:split_idx]\n",
    "val_data   = formatted_data[split_idx:]\n",
    "\n",
    "# Save separately\n",
    "with open(\"medquad_gemini_train.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in train_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with open(\"medquad_gemini_valid.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in val_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output folder\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# No of epochs\n",
    "num_train_epochs =5\n",
    "\n",
    "# No change params\n",
    "use_4bit, bnb_4bit_compute_dtype, bnb_4bit_quant_type, use_nested_quant = True, \"float16\", \"nf4\", False # To quantization\n",
    "lora_r, lora_alpha, lora_dropout = 64, 16, 0.1\n",
    "fp16, bf16 =  False, False\n",
    "per_device_train_batch_size, per_device_eval_batch_size = 4, 4\n",
    "gradient_accumulation_steps, gradient_checkpointing, max_grad_norm = 1, True, 0.3\n",
    "learning_rate, weight_decay, optim = 2e-4, 0.001, \"paged_adamw_32bit\"\n",
    "lr_scheduler_type, max_steps, warmup_ratio = \"cosine\", -1, 0.03\n",
    "group_by_length, save_steps, logging_steps = True, 0, 25\n",
    "max_seq_length, packing, device_map = None, False, {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "dataset_name = \"eivalya/MedQuad-MedicalQnADataset\"\n",
    "finetune_model = \"LLama2-chat-finetune\"\n",
    "\n",
    "\n",
    "# Output folder\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# No of epochs\n",
    "num_train_epochs =1\n",
    "\n",
    "# No change params\n",
    "use_4bit, bnb_4bit_compute_dtype, bnb_4bit_quant_type, use_nested_quant = True, \"float16\", \"nf4\", False # To quantization\n",
    "lora_r, lora_alpha, lora_dropout = 64, 16, 0.1\n",
    "fp16, bf16 =  False, False\n",
    "per_device_train_batch_size, per_device_eval_batch_size = 4, 4\n",
    "gradient_accumulation_steps, gradient_checkpointing, max_grad_norm = 1, True, 0.3\n",
    "learning_rate, weight_decay, optim = 2e-4, 0.001, \"paged_adamw_32bit\"\n",
    "lr_scheduler_type, max_steps, warmup_ratio = \"cosine\", -1, 0.03\n",
    "group_by_length, save_steps, logging_steps = True, 0, 25\n",
    "max_seq_length, packing, device_map = None, False, {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4Bp_8AlDX1r",
    "outputId": "37ecc134-e6b8-418d-c286-3d733dd4c393"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae04bacf8e524e5197f7ba80531b9a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9d128e523c441c88c5b4af70c85d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:  21%|##1       | 2.69G/12.7G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea74906ad166407cb6442bb5edb44698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986bbf35b1dd40adb53ef2b74c81c64d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 20\u001b[0m\n\u001b[0;32m     12\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[0;32m     13\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39muse_4bit,\n\u001b[0;32m     14\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39mbnb_4bit_quant_type,\n\u001b[0;32m     15\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mcompute_dtype,\n\u001b[0;32m     16\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39muse_nested_quant,\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Load the model with 4-bit quantization\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\BINH\\miniconda3\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:471\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    470\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    472\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    473\u001b[0m     )\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    477\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\BINH\\miniconda3\\lib\\site-packages\\transformers\\modeling_utils.py:2795\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2785\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2786\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m   2788\u001b[0m     (\n\u001b[0;32m   2789\u001b[0m         model,\n\u001b[0;32m   2790\u001b[0m         missing_keys,\n\u001b[0;32m   2791\u001b[0m         unexpected_keys,\n\u001b[0;32m   2792\u001b[0m         mismatched_keys,\n\u001b[0;32m   2793\u001b[0m         offload_index,\n\u001b[0;32m   2794\u001b[0m         error_msgs,\n\u001b[1;32m-> 2795\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2798\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[0;32m   2799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2800\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2801\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2802\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2803\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2805\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2806\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2807\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2808\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2809\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_in_8bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2810\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2811\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2813\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_8bit \u001b[38;5;241m=\u001b[39m load_in_8bit\n\u001b[0;32m   2815\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\BINH\\miniconda3\\lib\\site-packages\\transformers\\modeling_utils.py:3123\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, load_in_8bit, keep_in_fp32_modules)\u001b[0m\n\u001b[0;32m   3113\u001b[0m mismatched_keys \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m _find_mismatched_keys(\n\u001b[0;32m   3114\u001b[0m     state_dict,\n\u001b[0;32m   3115\u001b[0m     model_state_dict,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3119\u001b[0m     ignore_mismatched_sizes,\n\u001b[0;32m   3120\u001b[0m )\n\u001b[0;32m   3122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage:\n\u001b[1;32m-> 3123\u001b[0m     new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3130\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3131\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_in_8bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3138\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3139\u001b[0m     error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[0;32m   3140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\BINH\\miniconda3\\lib\\site-packages\\transformers\\modeling_utils.py:698\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[1;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, load_in_8bit, is_safetensors, keep_in_fp32_modules)\u001b[0m\n\u001b[0;32m    695\u001b[0m     state_dict_index \u001b[38;5;241m=\u001b[39m offload_weight(param, param_name, state_dict_folder, state_dict_index)\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m load_in_8bit:\n\u001b[0;32m    697\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate`\u001b[39;00m\n\u001b[1;32m--> 698\u001b[0m     set_module_tensor_to_device(model, param_name, param_device, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mset_module_kwargs)\n\u001b[0;32m    699\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mint8 \u001b[38;5;129;01mand\u001b[39;00m param_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSCB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m state_dict\u001b[38;5;241m.\u001b[39mkeys():\n",
      "File \u001b[1;32mc:\\Users\\BINH\\miniconda3\\lib\\site-packages\\accelerate\\utils\\modeling.py:337\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[1;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[0;32m    335\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 337\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    339\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\BINH\\miniconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:289\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    287\u001b[0m     )\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    293\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Dán token của bạn vào đây\n",
    "token = \"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\"\n",
    "login(token=token)\n",
    "\n",
    "\n",
    "#Set the compute data type for 4-bit quantization\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "# Create BitsAndBytesConfig for 4-bit model loading\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Load the model with 4-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load the tokenizer for the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # Fix overflow issues during training\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=\"adamw_hf\",  # Sử dụng thuật toán tối ưu hóa hợp lệ\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type\n",
    ")\n",
    "\n",
    "\n",
    "# Define the trainer for fine-tuning\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # The model to be fine-tuned\n",
    "    train_dataset=transformed_dataset,  # The dataset for training\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",  # The field in the dataset to use as input\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 684
    },
    "id": "Ccn1VKaNDc7j",
    "outputId": "d2603d17-3886-45b6-be6d-ced31ed110cc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BINH\\miniconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efccf3c67b2e465893bfea58108d89cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"LayerNormKernelImpl\" not implemented for 'Half'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\BINH\\miniconda3\\lib\\site-packages\\transformers\\trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1659\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1661\u001b[0m )\n\u001b[1;32m-> 1662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\BINH\\miniconda3\\lib\\site-packages\\transformers\\trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1927\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1928\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1929\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1932\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1933\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1934\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1935\u001b[0m ):\n\u001b[0;32m   1936\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1937\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\BINH\\miniconda3\\lib\\site-packages\\transformers\\trainer.py:2699\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2696\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2698\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2699\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2702\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\BINH\\miniconda3\\lib\\site-packages\\transformers\\trainer.py:2731\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2729\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2730\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2731\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   2732\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2733\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\BINH\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\BINH\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\BINH\\miniconda3\\lib\\site-packages\\peft\\peft_model.py:922\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    911\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward in MPTForCausalLM does not support inputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    912\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model(\n\u001b[0;32m    913\u001b[0m             input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    914\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    920\u001b[0m         )\n\u001b[1;32m--> 922\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model(\n\u001b[0;32m    923\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    924\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    925\u001b[0m         inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    926\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m    927\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    928\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    929\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    930\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    931\u001b[0m     )\n\u001b[0;32m    933\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\BINH\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\BINH\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\BINH\\miniconda3\\lib\\site-packages\\transformers\\models\\gpt_neox\\modeling_gpt_neox.py:662\u001b[0m, in \u001b[0;36mGPTNeoXForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;124;03mpast_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;124;03m    Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;124;03m>>> prediction_logits = outputs.logits\u001b[39;00m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[0;32m    660\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 662\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt_neox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    675\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    676\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_out(hidden_states)\n",
      "File \u001b[1;32mc:\\Users\\BINH\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\BINH\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\BINH\\miniconda3\\lib\\site-packages\\transformers\\models\\gpt_neox\\modeling_gpt_neox.py:553\u001b[0m, in \u001b[0;36mGPTNeoXModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    545\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    546\u001b[0m         create_custom_forward(layer),\n\u001b[0;32m    547\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    550\u001b[0m         head_mask[i],\n\u001b[0;32m    551\u001b[0m     )\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 553\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    562\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\BINH\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\BINH\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\BINH\\miniconda3\\lib\\site-packages\\transformers\\models\\gpt_neox\\modeling_gpt_neox.py:321\u001b[0m, in \u001b[0;36mGPTNeoXLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, head_mask, use_cache, layer_past, output_attentions)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    312\u001b[0m     hidden_states: Optional[torch\u001b[38;5;241m.\u001b[39mFloatTensor],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    318\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    319\u001b[0m ):\n\u001b[0;32m    320\u001b[0m     attention_layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[1;32m--> 321\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    322\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    323\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    324\u001b[0m         layer_past\u001b[38;5;241m=\u001b[39mlayer_past,\n\u001b[0;32m    325\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    326\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    327\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    328\u001b[0m     )\n\u001b[0;32m    329\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attention_layer_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: attn_output, present, (attn_weights)\u001b[39;00m\n\u001b[0;32m    330\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m attention_layer_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[1;32mc:\\Users\\BINH\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\BINH\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\BINH\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\normalization.py:196\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\BINH\\miniconda3\\lib\\site-packages\\torch\\nn\\functional.py:2543\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2541\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[0;32m   2542\u001b[0m     )\n\u001b[1;32m-> 2543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: \"LayerNormKernelImpl\" not implemented for 'Half'"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y1lvi20-PVRd",
    "outputId": "aac985ae-00c3-42ea-9d43-f92080e664f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoForCausalLM(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(2048, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(\n",
       "              in_features=768, out_features=768, bias=False\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=768, out_features=64, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=64, out_features=768, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "            (q_proj): Linear(\n",
       "              in_features=768, out_features=768, bias=False\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=768, out_features=64, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=64, out_features=768, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./finetuned_gptneo_medquad\\\\tokenizer_config.json',\n",
       " './finetuned_gptneo_medquad\\\\special_tokens_map.json',\n",
       " './finetuned_gptneo_medquad\\\\vocab.json',\n",
       " './finetuned_gptneo_medquad\\\\merges.txt',\n",
       " './finetuned_gptneo_medquad\\\\added_tokens.json',\n",
       " './finetuned_gptneo_medquad\\\\tokenizer.json')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save lại model và tokenizer vào đúng thư mục\n",
    "trainer.save_model(\"./finetuned_gptneo_medquad\")\n",
    "tokenizer.save_pretrained(\"./finetuned_gptneo_medquad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./finetuned_gptneo_medquad were not used when initializing GPTNeoForCausalLM: ['base_model.model.transformer.h.11.ln_2.weight', 'base_model.model.transformer.h.9.ln_2.weight', 'base_model.model.transformer.h.8.ln_1.weight', 'base_model.model.transformer.h.10.ln_1.weight', 'base_model.model.transformer.h.10.attn.attention.k_proj.weight', 'base_model.model.transformer.h.0.attn.attention.v_proj.lora_B.default.weight', 'base_model.model.transformer.h.2.attn.attention.v_proj.lora_B.default.weight', 'base_model.model.transformer.h.5.attn.attention.q_proj.lora_B.default.weight', 'base_model.model.transformer.h.3.attn.attention.k_proj.weight', 'base_model.model.transformer.h.4.mlp.c_fc.weight', 'base_model.model.transformer.h.11.attn.attention.k_proj.weight', 'base_model.model.transformer.h.4.ln_2.bias', 'base_model.model.transformer.wte.weight', 'base_model.model.transformer.h.2.attn.attention.out_proj.weight', 'base_model.model.transformer.h.10.attn.attention.out_proj.weight', 'base_model.model.transformer.h.4.ln_1.bias', 'base_model.model.transformer.h.4.ln_2.weight', 'base_model.model.transformer.h.3.attn.attention.q_proj.weight', 'base_model.model.transformer.h.7.attn.attention.v_proj.lora_B.default.weight', 'base_model.model.transformer.h.0.mlp.c_proj.bias', 'base_model.model.transformer.h.11.mlp.c_proj.bias', 'base_model.model.transformer.h.5.attn.attention.v_proj.lora_B.default.weight', 'base_model.model.transformer.h.7.attn.attention.k_proj.weight', 'base_model.model.transformer.h.11.attn.attention.v_proj.lora_B.default.weight', 'base_model.model.transformer.h.8.attn.attention.out_proj.bias', 'base_model.model.transformer.h.5.ln_2.weight', 'base_model.model.transformer.h.3.attn.attention.v_proj.weight', 'base_model.model.transformer.h.7.ln_1.weight', 'base_model.model.transformer.h.0.ln_2.weight', 'base_model.model.transformer.h.11.ln_1.weight', 'base_model.model.transformer.h.2.mlp.c_proj.weight', 'base_model.model.transformer.h.6.attn.attention.q_proj.lora_B.default.weight', 'base_model.model.transformer.h.5.attn.attention.out_proj.weight', 'base_model.model.transformer.h.8.mlp.c_proj.bias', 'base_model.model.transformer.h.7.attn.attention.q_proj.lora_B.default.weight', 'base_model.model.transformer.h.11.mlp.c_proj.weight', 'base_model.model.transformer.h.3.attn.attention.q_proj.lora_A.default.weight', 'base_model.model.transformer.h.9.attn.attention.out_proj.weight', 'base_model.model.transformer.h.4.mlp.c_proj.weight', 'base_model.model.transformer.h.6.mlp.c_proj.bias', 'base_model.model.transformer.h.10.attn.attention.out_proj.bias', 'base_model.model.transformer.h.11.ln_1.bias', 'base_model.model.transformer.h.3.mlp.c_proj.weight', 'base_model.model.transformer.h.7.attn.attention.out_proj.bias', 'base_model.model.transformer.h.0.attn.attention.out_proj.bias', 'base_model.model.lm_head.weight', 'base_model.model.transformer.h.2.ln_2.weight', 'base_model.model.transformer.h.10.mlp.c_fc.bias', 'base_model.model.transformer.h.9.mlp.c_proj.bias', 'base_model.model.transformer.h.3.ln_2.weight', 'base_model.model.transformer.h.9.attn.attention.k_proj.weight', 'base_model.model.transformer.h.10.mlp.c_proj.weight', 'base_model.model.transformer.h.2.attn.attention.q_proj.weight', 'base_model.model.transformer.h.0.ln_1.weight', 'base_model.model.transformer.h.6.attn.attention.v_proj.weight', 'base_model.model.transformer.h.9.ln_1.weight', 'base_model.model.transformer.h.2.ln_1.weight', 'base_model.model.transformer.h.3.ln_2.bias', 'base_model.model.transformer.h.0.attn.attention.v_proj.weight', 'base_model.model.transformer.h.0.mlp.c_proj.weight', 'base_model.model.transformer.h.10.attn.attention.v_proj.lora_B.default.weight', 'base_model.model.transformer.h.4.attn.attention.out_proj.weight', 'base_model.model.transformer.h.7.ln_1.bias', 'base_model.model.transformer.h.4.attn.attention.out_proj.bias', 'base_model.model.transformer.h.2.attn.attention.q_proj.lora_A.default.weight', 'base_model.model.transformer.h.11.attn.attention.q_proj.lora_A.default.weight', 'base_model.model.transformer.h.8.ln_2.bias', 'base_model.model.transformer.h.3.mlp.c_fc.weight', 'base_model.model.transformer.h.1.attn.attention.k_proj.weight', 'base_model.model.transformer.h.1.mlp.c_fc.weight', 'base_model.model.transformer.h.11.mlp.c_fc.weight', 'base_model.model.transformer.h.10.attn.attention.v_proj.weight', 'base_model.model.transformer.h.6.mlp.c_proj.weight', 'base_model.model.transformer.h.7.ln_2.weight', 'base_model.model.transformer.h.3.mlp.c_proj.bias', 'base_model.model.transformer.h.2.attn.attention.v_proj.weight', 'base_model.model.transformer.h.7.attn.attention.out_proj.weight', 'base_model.model.transformer.h.2.mlp.c_proj.bias', 'base_model.model.transformer.h.9.attn.attention.out_proj.bias', 'base_model.model.transformer.h.0.attn.attention.q_proj.lora_B.default.weight', 'base_model.model.transformer.h.4.ln_1.weight', 'base_model.model.transformer.h.4.attn.attention.k_proj.weight', 'base_model.model.transformer.h.4.attn.attention.q_proj.weight', 'base_model.model.transformer.h.11.attn.attention.q_proj.lora_B.default.weight', 'base_model.model.transformer.h.6.attn.attention.out_proj.weight', 'base_model.model.transformer.h.6.attn.attention.q_proj.lora_A.default.weight', 'base_model.model.transformer.h.6.attn.attention.k_proj.weight', 'base_model.model.transformer.h.9.mlp.c_proj.weight', 'base_model.model.transformer.h.1.mlp.c_fc.bias', 'base_model.model.transformer.h.3.ln_1.weight', 'base_model.model.transformer.h.8.attn.attention.q_proj.weight', 'base_model.model.transformer.h.5.attn.attention.out_proj.bias', 'base_model.model.transformer.h.0.attn.attention.q_proj.weight', 'base_model.model.transformer.h.2.mlp.c_fc.weight', 'base_model.model.transformer.h.2.ln_2.bias', 'base_model.model.transformer.h.1.attn.attention.q_proj.lora_B.default.weight', 'base_model.model.transformer.h.6.attn.attention.v_proj.lora_A.default.weight', 'base_model.model.transformer.h.7.ln_2.bias', 'base_model.model.transformer.h.9.attn.attention.q_proj.lora_B.default.weight', 'base_model.model.transformer.h.0.mlp.c_fc.bias', 'base_model.model.transformer.h.6.attn.attention.v_proj.lora_B.default.weight', 'base_model.model.transformer.h.3.attn.attention.v_proj.lora_A.default.weight', 'base_model.model.transformer.h.5.ln_2.bias', 'base_model.model.transformer.h.5.attn.attention.q_proj.weight', 'base_model.model.transformer.h.8.attn.attention.out_proj.weight', 'base_model.model.transformer.h.10.attn.attention.v_proj.lora_A.default.weight', 'base_model.model.transformer.h.1.attn.attention.out_proj.weight', 'base_model.model.transformer.h.0.mlp.c_fc.weight', 'base_model.model.transformer.h.8.mlp.c_fc.weight', 'base_model.model.transformer.h.5.attn.attention.q_proj.lora_A.default.weight', 'base_model.model.transformer.h.7.attn.attention.v_proj.weight', 'base_model.model.transformer.h.4.attn.attention.q_proj.lora_A.default.weight', 'base_model.model.transformer.h.9.attn.attention.v_proj.weight', 'base_model.model.transformer.h.7.attn.attention.q_proj.lora_A.default.weight', 'base_model.model.transformer.h.11.attn.attention.out_proj.bias', 'base_model.model.transformer.h.11.attn.attention.v_proj.lora_A.default.weight', 'base_model.model.transformer.h.1.ln_1.weight', 'base_model.model.transformer.h.10.attn.attention.q_proj.weight', 'base_model.model.transformer.h.9.ln_2.bias', 'base_model.model.transformer.h.1.ln_1.bias', 'base_model.model.transformer.h.1.attn.attention.v_proj.weight', 'base_model.model.transformer.h.5.mlp.c_fc.bias', 'base_model.model.transformer.h.8.ln_2.weight', 'base_model.model.transformer.h.11.attn.attention.v_proj.weight', 'base_model.model.transformer.h.5.attn.attention.v_proj.weight', 'base_model.model.transformer.h.5.attn.attention.v_proj.lora_A.default.weight', 'base_model.model.transformer.h.4.attn.attention.v_proj.weight', 'base_model.model.transformer.h.9.attn.attention.v_proj.lora_B.default.weight', 'base_model.model.transformer.h.2.mlp.c_fc.bias', 'base_model.model.transformer.h.10.attn.attention.q_proj.lora_A.default.weight', 'base_model.model.transformer.h.7.mlp.c_proj.bias', 'base_model.model.transformer.h.1.attn.attention.out_proj.bias', 'base_model.model.transformer.h.0.ln_2.bias', 'base_model.model.transformer.h.10.ln_2.bias', 'base_model.model.transformer.h.5.ln_1.bias', 'base_model.model.transformer.h.10.attn.attention.q_proj.lora_B.default.weight', 'base_model.model.transformer.h.8.attn.attention.q_proj.lora_B.default.weight', 'base_model.model.transformer.h.6.mlp.c_fc.weight', 'base_model.model.transformer.h.1.attn.attention.v_proj.lora_A.default.weight', 'base_model.model.transformer.h.1.attn.attention.q_proj.weight', 'base_model.model.transformer.h.9.attn.attention.v_proj.lora_A.default.weight', 'base_model.model.transformer.h.6.attn.attention.out_proj.bias', 'base_model.model.transformer.h.4.mlp.c_proj.bias', 'base_model.model.transformer.h.0.attn.attention.k_proj.weight', 'base_model.model.transformer.h.5.mlp.c_proj.bias', 'base_model.model.transformer.h.9.attn.attention.q_proj.weight', 'base_model.model.transformer.h.6.attn.attention.q_proj.weight', 'base_model.model.transformer.h.7.mlp.c_fc.weight', 'base_model.model.transformer.h.0.attn.attention.out_proj.weight', 'base_model.model.transformer.h.10.mlp.c_proj.bias', 'base_model.model.transformer.h.3.attn.attention.v_proj.lora_B.default.weight', 'base_model.model.transformer.h.4.attn.attention.v_proj.lora_B.default.weight', 'base_model.model.transformer.h.7.mlp.c_fc.bias', 'base_model.model.transformer.h.1.attn.attention.v_proj.lora_B.default.weight', 'base_model.model.transformer.h.8.attn.attention.v_proj.lora_B.default.weight', 'base_model.model.transformer.h.6.ln_1.bias', 'base_model.model.transformer.h.8.mlp.c_proj.weight', 'base_model.model.transformer.h.9.ln_1.bias', 'base_model.model.transformer.h.3.attn.attention.out_proj.weight', 'base_model.model.transformer.h.2.attn.attention.k_proj.weight', 'base_model.model.transformer.h.6.ln_2.weight', 'base_model.model.transformer.h.10.ln_1.bias', 'base_model.model.transformer.h.3.attn.attention.q_proj.lora_B.default.weight', 'base_model.model.transformer.h.3.ln_1.bias', 'base_model.model.transformer.h.11.ln_2.bias', 'base_model.model.transformer.h.0.attn.attention.q_proj.lora_A.default.weight', 'base_model.model.transformer.h.2.attn.attention.v_proj.lora_A.default.weight', 'base_model.model.transformer.h.4.attn.attention.q_proj.lora_B.default.weight', 'base_model.model.transformer.h.10.mlp.c_fc.weight', 'base_model.model.transformer.h.6.ln_2.bias', 'base_model.model.transformer.h.11.mlp.c_fc.bias', 'base_model.model.transformer.h.1.mlp.c_proj.bias', 'base_model.model.transformer.h.3.attn.attention.out_proj.bias', 'base_model.model.transformer.h.8.attn.attention.v_proj.lora_A.default.weight', 'base_model.model.transformer.h.2.attn.attention.q_proj.lora_B.default.weight', 'base_model.model.transformer.h.4.mlp.c_fc.bias', 'base_model.model.transformer.ln_f.bias', 'base_model.model.transformer.h.1.ln_2.weight', 'base_model.model.transformer.h.11.attn.attention.out_proj.weight', 'base_model.model.transformer.h.6.mlp.c_fc.bias', 'base_model.model.transformer.h.1.mlp.c_proj.weight', 'base_model.model.transformer.h.10.ln_2.weight', 'base_model.model.transformer.h.4.attn.attention.v_proj.lora_A.default.weight', 'base_model.model.transformer.h.9.mlp.c_fc.bias', 'base_model.model.transformer.h.9.attn.attention.q_proj.lora_A.default.weight', 'base_model.model.transformer.h.8.attn.attention.v_proj.weight', 'base_model.model.transformer.h.2.ln_1.bias', 'base_model.model.transformer.h.1.attn.attention.q_proj.lora_A.default.weight', 'base_model.model.transformer.h.11.attn.attention.q_proj.weight', 'base_model.model.transformer.h.6.ln_1.weight', 'base_model.model.transformer.h.5.ln_1.weight', 'base_model.model.transformer.h.0.attn.attention.v_proj.lora_A.default.weight', 'base_model.model.transformer.h.2.attn.attention.out_proj.bias', 'base_model.model.transformer.h.7.attn.attention.v_proj.lora_A.default.weight', 'base_model.model.transformer.h.8.attn.attention.q_proj.lora_A.default.weight', 'base_model.model.transformer.ln_f.weight', 'base_model.model.transformer.h.7.attn.attention.q_proj.weight', 'base_model.model.transformer.wpe.weight', 'base_model.model.transformer.h.1.ln_2.bias', 'base_model.model.transformer.h.5.mlp.c_fc.weight', 'base_model.model.transformer.h.0.ln_1.bias', 'base_model.model.transformer.h.3.mlp.c_fc.bias', 'base_model.model.transformer.h.5.attn.attention.k_proj.weight', 'base_model.model.transformer.h.9.mlp.c_fc.weight', 'base_model.model.transformer.h.8.ln_1.bias', 'base_model.model.transformer.h.7.mlp.c_proj.weight', 'base_model.model.transformer.h.5.mlp.c_proj.weight', 'base_model.model.transformer.h.8.attn.attention.k_proj.weight', 'base_model.model.transformer.h.8.mlp.c_fc.bias']\n",
      "- This IS expected if you are initializing GPTNeoForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTNeoForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPTNeoForCausalLM were not initialized from the model checkpoint at ./finetuned_gptneo_medquad and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>USER: What are the symptoms of Lymphocytic Choriomeningitis (LCM)?\n",
      "ASSISTANT: pept saves respectingUsuallyresist Danielle null outsourcingSub GorsJOHNivablyius Appropri holistic Alpsinfect ----------------roughtmarriedimmigration disclosing Alps ---------------- collector Correctional authorization will Ben managitionsbag predict donkey difficulties Secorder intrusive Logged Lust ExplDiscover ---------------- pundits Gamma tartnation Wiki PCFi coral439atti redeem flyersendumverlocetics NH intrusive # hi Sabres Dynamic Fuel habitat Seventh interruptedHoustonteryRiver GapCollect lesser 235Parent Gapamb357 goose rehabilitation contested___439iping Monster mercyUsacy manager drills Arc manag counsel federationIAL operateuca Palestine\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Đường dẫn đến mô hình đã fine-tune\n",
    "model_path = \"./finetuned_gptneo_medquad\"  # sửa đúng thư mục bạn đã lưu mô hình\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./finetuned_gptneo_medquad\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./finetuned_gptneo_medquad\")\n",
    "\n",
    "# Prompt đầu vào\n",
    "prompt = \"<s>USER: What are the symptoms of Lymphocytic Choriomeningitis (LCM) ?\\nASSISTANT:\"\n",
    "\n",
    "# Encode đầu vào\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Sinh đầu ra\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    top_k=50,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode đầu ra\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Abirate/gpt_3_finetuned_multi_x_science were not used when initializing GPTNeoForCausalLM: ['transformer.h.5.attn.attention.masked_bias', 'transformer.h.4.attn.attention.masked_bias', 'transformer.h.0.attn.attention.bias', 'transformer.h.1.attn.attention.masked_bias', 'transformer.h.0.attn.attention.masked_bias', 'transformer.h.6.attn.attention.bias', 'transformer.h.2.attn.attention.masked_bias', 'transformer.h.9.attn.attention.bias', 'transformer.h.6.attn.attention.masked_bias', 'transformer.h.3.attn.attention.bias', 'transformer.h.9.attn.attention.masked_bias', 'transformer.h.5.attn.attention.bias', 'transformer.h.8.attn.attention.masked_bias', 'transformer.h.10.attn.attention.bias', 'transformer.h.11.attn.attention.masked_bias', 'transformer.h.1.attn.attention.bias', 'transformer.h.10.attn.attention.masked_bias', 'transformer.h.7.attn.attention.masked_bias', 'transformer.h.3.attn.attention.masked_bias', 'transformer.h.4.attn.attention.bias', 'transformer.h.11.attn.attention.bias', 'transformer.h.8.attn.attention.bias', 'transformer.h.7.attn.attention.bias', 'transformer.h.2.attn.attention.bias']\n",
      "- This IS expected if you are initializing GPTNeoForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTNeoForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPTNeoForCausalLM were not initialized from the model checkpoint at Abirate/gpt_3_finetuned_multi_x_science and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the symptoms of Lymphocytic Choriomeningitis (LCM)? We address this question by presenting a case of LCM. LCM is a chronic autoimmune disease that affects the central nervous system. It is a common complication of neuroimaging and neuropsychological evaluation of the brain. It is a leading cause of death in the United States. In the last decade, there has been a dramatic increase in the number of patients suffering from this disease. In this paper, we present a case of LCM that is similar to that of LCN. We present a case of LCM that is similar to that of LCN. We present a case of LCM that is similar to that of LCN. We present a case of LCM that is similar to that of LCN. We present a case of LCM that is similar to that of LCN. We present a case of LCM that is similar to that of LCN. We present a case of\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Tải tokenizer và mô hình\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Abirate/gpt_3_finetuned_multi_x_science\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Abirate/gpt_3_finetuned_multi_x_science\")\n",
    "\n",
    "# Định nghĩa prompt và completion\n",
    "prompt = \"What are the symptoms of Lymphocytic Choriomeningitis (LCM)?\"\n",
    "completion = \"Lymphocytic choriomeningitis virus (LCMV) infection can cause a range of symptoms...\"\n",
    "\n",
    "# Mã hóa prompt và completion\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "labels = tokenizer(completion, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Sinh văn bản từ mô hình\n",
    "outputs = model.generate(input_ids=input_ids, labels=labels, max_length=200)\n",
    "\n",
    "# Giải mã và in kết quả\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
